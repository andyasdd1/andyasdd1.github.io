{
  "Personal Info": {
    "Prefix": "Mister",
    "Legal first name / first given name": "Zihan",
    "Legal last name / family name / surname": "Zhou",
    "Birthdate": "01/26/2000",
    "Birth City": "ZhangJiaGang",
    "Birth Region": "Jiangsu",
    "Birth Country": "China",
    "Legal Sex": "Male",
    "Gender Identity": "Cisgender (Non-trans) man/male",
    "Personal Pronouns": "He/Him/His",
    "Marital Status": "Single",
    "Dependent Children": "No",
    "Transgender": "No",
    "Sexual Orientation": "Heterosexual or Straight",
    "Veteran": "No",
    "Spouse/dependent of Veteran": "No"
  },
  "Contact Info": {
    "Personal Email": "andyjobs.zhou@gmail.com",
    "Mobile Phone": "3158781254",
    "Mobile Opt-In (text message)": "True",
    "Text Message": "",
    "WeChat ID": "",
    "Mailing Street Address": "1082 commonwealth Ave",
    "Mailing City": "Boston",
    "Mailing State/Region": "MA",
    "Mailing Zip": "02215",
    "Mailing Country": "United states",
    "Mailing Location": "1082 commonwealth Ave, Boston MA, 02215",
    "Mailing Address Valid Until": "August 2026",
    "Permanent Street Address": "HuanCheng north road 26, ChengYiHuaYuan, Apt3 room 906 ",
    "Permanent City": "Jiangyin",
    "Permanent State/Region": "Jiangsu",
    "Permanent Region/State": "",
    "Permanent Province": "Jiangsu",
    "Permanent Zip": "14456",
    "Permanent Country": "China",
    "Permanent Location": "HuanCheng north road 26, ChengYiHuaYuan, Apt3 room 906, 14456, Jiangyin, Jiangsu, China"
  },
  "Citizenship & Residency": {
    "Citizenship Status": "Foreign National",
    "Citizenship status": "International",
    "Country of Citizenship": "China",
    "Country of Residence": "China",
    "Current residency/visa status": "F1 Student Visa",
    "Hold citizenship in another country": "No",
    "California resident": "No"
  },
  "Ethnicity & Language": {
    "Hispanic": "No",
    "Asian": "true",
    "China (Ethnicity)": "true",
    "Native Language": "Chinese",
    "Primary Language": "Mandarin",
    "Foreign Language 1": "English"
  },
  "Family & Background": {
    "Siblings": "No",
    "Listed as dependent on tax return": "No",
    "First Generation": "Yes",
    "Parent 1 Education": "High School",
    "Parent 2 Education": "High School"
  },
  "Program of Interest": {
    "Area of Study": "Computer Science",
    "Select Field of Study": "Computer Science",
    "Select Degree": "Master",
    "Degree objective": "PhD",
    "Proposed date of entrance": "Fall 2026",
    "First research field of interest": "Machine Learning",
    "Second research field of interest (optional)": "Computer Vision",
    "Third research field of interest (optional)":"Reinforcement Learning",
    "Most Interested Sub-area": "Computer Vision",
    "Interested Sub-area": "Reinforcement Learning, Computer Vision, Machine Learning, Large Language model, Robotics, Uncrewed & Autonomous Systems",
    "Somewhat Interested Sub-area": "Data Science, Data analysis",
    "Choice One (Research Interest)": "Computer Vision",
    "Choice Two (Research Interest)": "Reinforcement Learning",
    "Interested in other programs": "No",
    "Considered at master's level": "No",
    "Considered by Center for Statistics and Machine Learning": "Yes",
    "Interested Division - Engineering": "Yes",
    "Considered for GSP": "Yes"
  },
  "Education History": {
    "Education 1: Institution Name": "Hobart and William Smith Colleges",
    "Education 1: Country": "United States",
    "Education 1: State": "NY",
    "Education 1: City": "Geneva",
    "Education 1: Start Date": "8/25/2019",
    "Education 1: End Date": "5/20/2023",
    "Education 1: Graduation Date": "5/18/2023",
    "Education 1: Major": "Computer Science",
    "Education 1: Degree": "Bachelor of Science",
    "Education 1: GPA": "3.31",
    "Education 1: School Not Found": "",
    "Education 2: Institution Name": "Boston University",
    "Education 2: Country": "United States",
    "Education 2: State": "MA",
    "Education 2: City": "Boston",
    "Education 2: Start Date": "9/01/2023",
    "Education 2: End Date": "5/19/2025",
    "Education 2: Degree": "Master of Science",
    "Education 2: Graduation Date": "5/26/2025",
    "Education 2: Major": "Computer Science",
    "Education 2: GPA": "3.15",
    "Education 2: School Not Found": "",
    "Education 3: Institution Name": "",
    "Education 3: Country": "",
    "Education 3: State": "",
    "Education 3: City": "",
    "Education 3: Start Date": "",
    "Education 3: End Date": "",
    "Education 3: GPA": "",
    "Education 3: School Not Found": "",
    "Education 4: Institution Name": "",
    "Education 4: Country": "",
    "Education 4: State": "",
    "Education 4: City": "",
    "Education 4: Start Date": "",
    "Education 4: End Date": "",
    "Education 4: GPA": "",
    "Education 5: Institution Name": "",
    "Education 5: Degree": "",
    "Education 5: Dates Attended": "",
    "Gaps in education history": ""
  },
  "Research Experience": {
    "Research 1: Start Date": "August 1st 2024",
    "Research 1: Months of Prior Full-Time Research Exp": "5",
    "Research 1: Months of Prior Part-Time Research Exp": "17",
    "Research 1: Research Topic": "Pose Estimation, Gradient Early stopping, Computer vision",
    "Research 1: Organization Name": "Boston University",
    "Research 1: Supervisor First Name": "Reza",
    "Research 1: Supervisor Last Name": "Rawassizadeh",
    "Research 1: Summary": "My overarching goal, which I established in late 2024, is to build a complete, end-to-end 'Camera to 3D Human Model' pipeline. I am designing this system to take 2D video, extract human pose, reconstruct it in 3D, and retarget the motion onto a rigged character in Blender (.blend files). My work has progressed through several distinct phases: I began by using OpenPose (which I ran on an Ubuntu VM to bypass Windows dependency issues) for initial 2D keypoint extraction. Concurrently, I explored practical pose matching algorithms using MediaPipe, Dynamic Programming (DTW), and k-NN. Seeking higher fidelity, I attempted to integrate Detectron2 (DensePose)—but encountered a 'filename or extension too long' error on Windows—and I also conducted detailed research on Meta's Sapiens model as a potential upgrade. After researching optimization strategies like 'Controlled Knowledge Distillation,' my current work has now returned to MediaPipe; I am now practically implementing this optimization phase by using its online tools to fine-tune a model with Lora-C, all while maintaining my strong focus on physical realism and automation.",
    "Research 2: Research Topic": "Fedrated Learning",
    "Research 2: Organization Name": "Boston University",
    "Research 2: Supervisor First Name": "Avi",
    "Research 2: Supervisor Last Name": "Mohan",
    "Research 2: Start Date": "February 1st 2025",
    "Research 2: Months of Prior Full-Time Research Exp": "5",
    "Research 2: Months of Prior Part-Time Research Exp": "9",
    "Research 2: Summary": "Enhancing Server-Local Model Generalization on Heterogeneous Data through Feature Learning Analysis and Benchmarking. I validated federated learning convergence theory by running two complementary research tracks: first, I built UltraFlwr, a custom framework integrating Flower and YOLOv8 for object detection on COCO/VisDrone datasets, where I discovered my COCO subset only had 2 vehicle classes (cars/motorcycles) so I created car-focused vs motorcycle-focused heterogeneous clients and found FedAvg achieved 12.3% better performance than FedSGD with 80% fewer communication rounds, though federated training had 52% worse performance than centralized training, quantifying the real cost of heterogeneity. Second, I ran controlled experiments with synthetic datasets at four heterogeneity levels (λ from 12 to 2185) using simple neural networks, systematically testing the theory's predictions about learning rates, local epochs, and gradient diversity - confirming the theory is directionally correct but discovering its learning rate bounds are 5-10× too conservative due to implicit regularization from Xavier init, ReLU, and gradient clipping. My most important finding came when I repeated experiments without Batch Normalization: WITH BatchNorm, local updates (E=5) beat synchronous training (E=1) by 5.5%, but WITHOUT BatchNorm, the result completely flipped with E=1 beating E=5 by 13% - an 18.6 percentage point swing - because BatchNorm prevents gradient diversity from exploding during training (λ decreased 371→111 with BN, but exploded 138→996 without BN). This proves Batch Normalization is essential for local updates to outperform synchronous methods in heterogeneous federated learning, which is completely missing from the original theory - that's my novel contribution that bridges theory and practice, establishing that modern normalization techniques aren't optional optimizations but fundamental enablers of the variance reduction benefits that make federated learning work."
  },
  "Publications": {
    "ORCID": "",
    "Publication 1: Title": "GradES: Significantly Faster Training in Transformers with Gradient-Based Early Stopping",
    "Publication 1: Source": "https://arxiv.org/abs/2509.01842",
    "Publication 1: Date": "Mon, 1 Sep 2025",
    "Publication 1: Summary": "Early stopping monitors global validation loss and halts all parameter updates simultaneously, which is computationally costly for large transformers due to the extended time required for validation inference. We propose \textit{GradES}, a novel gradient-based early stopping approach that operates within transformer components (attention projections and Feed-Forward layer matrices). We found that different components converge at varying rates during fine-tuning for both language and vision-language models. \textit{GradES} tracks the magnitude of gradient changes in backpropagation for these matrices during training. When a projection matrix's magnitude of gradient changes fall below a convergence threshold , we exclude that projection matrix from further updates individually, eliminating costly validation passes while allowing slow converging matrices to continue learning. \textit{GradES} speeds up training time by 1.57--7.22 while simultaneously enhancing generalization through early prevention of overfitting, resulting in 1.2% higher average accuracy in language tasks and 3.88% on multimodal benchmarks."
  },
  "Additional Info": {
    "Disciplinary Violation": "No",
    "Criminal History": "No",
    "Submitting documents under different name": "No",
    "Previous Applicant": "No",
    "Current/former student": "No",
    "Current employee": "No",
    "Dependent of employee": "No",
    "Fee waiver code": "No",
    "Fullbright": "No",
    "AmeriCorps": "No",
    "Aevium Tuition Grant": "No",
    "Ronald E. McNair": "No",
    "Global Pathways": "No",
    "Parent and Family Scholarship": "No",
    "Channel Partner": "No",
    "Applying for external fellowship": "No",
    "Awarded external fellowship": "No",
    "Interested in Horizon Fellows Program": "Yes"
  }
}